{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vbfJn5OJhFhd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "from string import punctuation\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from scipy.spatial import distance\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim.models import Word2Vec\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oWel07wqzEFQ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "DEVICE = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NZGW50dChXIT"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self, folders, train=True):\n",
        "        self.train = train\n",
        "        self._folders = folders\n",
        "        self._embeddings_files = torch.tensor([])\n",
        "        self._embeddings_plagiat1 = torch.tensor([])\n",
        "        self._embeddings_plagiat2 = torch.tensor([])\n",
        "        self.tk = WordPunctTokenizer()\n",
        "        self._all_data = []\n",
        "        self.files = []\n",
        "        self.plagiat1 = []\n",
        "        self.plagiat2 = []\n",
        "        self.test_sample = torch.tensor([])\n",
        "      \n",
        "    def _clean_punctuation(self, punct_lst:str = punctuation):\n",
        "        for letter in punct_lst:\n",
        "            self.text = [line.replace(letter,  '')  for line in self.text]\n",
        "\n",
        "    def _get_vec(self, txt: List[str]): \n",
        "        vec = torch.tensor([[0] * 300])\n",
        "        for word in txt:\n",
        "            vec += self._model[word]\n",
        "        return vec/len(txt) if len(txt)!=0 else vec\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "    def _preprocess(self):\n",
        "        if self.train:\n",
        "          self.tmp_folder = self.files\n",
        "          for idx,folder in enumerate(self._folders):\n",
        "              for j,filename in enumerate(os.listdir(folder)):\n",
        "                  full_filename = os.path.join(folder, filename)\n",
        "                  self.text = open(full_filename, 'r+').readlines()\n",
        "                  self.text = [line[:-2].lower() for line in self.text]\n",
        "                  self._clean_punctuation()\n",
        "                  self.text = [self.tk.tokenize(row) for row in self.text]\n",
        "                  self._all_data += self.text\n",
        "                  self.tmp_folder.append(self.text)\n",
        "              if idx == 0:\n",
        "                  self.tmp_folder = self.plagiat1\n",
        "              elif idx == 1:\n",
        "                  self.tmp_folder = self.plagiat2\n",
        "\n",
        "          self.plagiat1 = self.plagiat1[1:]\n",
        "          self.plagiat2 = self.plagiat2[1:]\n",
        "          self.lst_folders = [self.files, self.plagiat1, self.plagiat2]\n",
        "          self.lst_tensors = [self._embeddings_files, self._embeddings_plagiat1, self._embeddings_plagiat2]\n",
        "\n",
        "\n",
        "          self._model = Word2Vec(\n",
        "            sentences = self._all_data,\n",
        "            min_count=1,\n",
        "            window=5,\n",
        "            size=300,\n",
        "            negative=10,\n",
        "            alpha=0.03,\n",
        "            min_alpha=0.0007,\n",
        "            sample=6e-5,\n",
        "            sg=1,\n",
        "            workers=4\n",
        "            )\n",
        "          \n",
        "          self._model.save('Word2Vec.model')\n",
        "\n",
        "          for idx, folder in enumerate(self.lst_folders):\n",
        "            self.tmp_folder = torch.tensor([])\n",
        "            for j,text in enumerate(folder):\n",
        "                chain_text = list(itertools.chain(*text))\n",
        "                self._emb = self._get_vec(chain_text)\n",
        "                self.tmp_folder = torch.cat((self.tmp_folder,self._emb))\n",
        "            self.lst_tensors[idx] = self.tmp_folder\n",
        "        \n",
        "        else:\n",
        "            folder = self._folders[-1]\n",
        "            self._model = Word2Vec.load('Word2Vec.model')\n",
        "            for filename in os.listdir(folder):\n",
        "                  full_filename = os.path.join(folder, filename)\n",
        "                  self.text = open(full_filename, 'r+').readlines()\n",
        "                  self.text = [line[:-2].lower() for line in self.text]\n",
        "                  self._clean_punctuation()\n",
        "                  self.text = [self.tk.tokenize(row) for row in self.text]\n",
        "                  self.vec = self._get_vec(self.text)\n",
        "                  self.test_sample = torch.cat((self.test_sample, self.vec))\n",
        "\n",
        "    def preprocessing(self):\n",
        "        self._preprocess()\n",
        "        return self.lst_tensors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "           \n",
        "\n",
        "       \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Y31EGpAXWEQC"
      },
      "outputs": [],
      "source": [
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, files:torch.Tensor, plagiat1:torch.Tensor, plagiat2:torch.Tensor):\n",
        "        self.files = files\n",
        "        self.plagiat = [plagiat1, plagiat2]\n",
        "        self.length = len(files)\n",
        "        self.anchor_box = torch.tensor([])\n",
        "        self.positive_box = torch.tensor([])\n",
        "        self.negative_box = torch.tensor([])\n",
        "        self.make_samples()\n",
        "\n",
        "    def make_samples(self):\n",
        "        for idx, vec in enumerate(self.files):\n",
        "\n",
        "\n",
        "            negative_files = torch.cat((self.files[:idx],self.files[idx+1:]))\n",
        "            negative_plagiat1 = torch.cat((self.plagiat[0][:idx],self.plagiat[0][idx+1:]))\n",
        "            negative_plagiat2 = torch.cat((self.plagiat[1][:idx],self.plagiat[1][idx+1:]))\n",
        "            all_negative = torch.cat((negative_files,negative_plagiat1,negative_plagiat2))\n",
        "\n",
        "\n",
        "            for plag in range(2):\n",
        "                anchor = vec.reshape(1,-1)\n",
        "                positive = self.plagiat[plag][idx].reshape(1,-1)\n",
        "                negative_idx = random.randint(0,len(all_negative)-1)\n",
        "                negative = all_negative[negative_idx].reshape(1,-1)\n",
        "                self.anchor_box = torch.cat((self.anchor_box,anchor))\n",
        "                self.positive_box = torch.cat((self.positive_box,positive))\n",
        "                self.negative_box = torch.cat((self.negative_box,negative))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchor_box)\n",
        "     \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.vstack((self.anchor_box[idx], self.positive_box[idx], self.negative_box[idx]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CEFrEP3ARofb"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dataset=None):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(300,512)\n",
        "        self.fc2 = nn.Linear(512,1024)\n",
        "        self.fc3 = nn.Linear(1024,4096)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fpGjh2BL7eOk"
      },
      "outputs": [],
      "source": [
        "lst = ['/content/files','/content/plagiat1','/content/plagiat2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jAUpRloukKKS"
      },
      "outputs": [],
      "source": [
        "pp = Preprocessor(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlWK079gYwyX",
        "outputId": "5639a2ca-1e26-42b7-d23f-fe4a8f96507e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4feca3fd773c>:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  vec += self._model[word]\n"
          ]
        }
      ],
      "source": [
        "files, plagiat1, plagiat2 = pp.preprocessing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jbkGPwE1Y_WB"
      },
      "outputs": [],
      "source": [
        "ds = TripletDataset(files, plagiat1, plagiat2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiBM1fh6RPZt",
        "outputId": "abb8a9ae-7554-4652-8517-7d8daaae5a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:02<11:01,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9992, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 11/300 [00:20<09:07,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9608, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 21/300 [00:38<08:06,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9142, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 31/300 [00:57<08:13,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8866, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▎        | 41/300 [01:15<07:59,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8119, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [01:33<07:05,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8249, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 61/300 [01:50<06:36,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7252, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▎       | 71/300 [02:09<08:19,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8036, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 81/300 [02:31<07:46,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7619, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 91/300 [02:54<08:07,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7734, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [03:18<08:20,  2.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6635, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 111/300 [03:41<07:06,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8038, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 121/300 [04:05<07:00,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6581, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▎     | 131/300 [04:29<06:58,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5303, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 141/300 [04:54<06:51,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6715, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [05:19<06:13,  2.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6335, grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████▏    | 154/300 [05:26<05:58,  2.46s/it]"
          ]
        }
      ],
      "source": [
        "net = Net(dataset=ds)\n",
        "errors = []\n",
        "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "length_sample = len(dl)\n",
        "criterion = nn.TripletMarginWithDistanceLoss(distance_function=(lambda x,y : 1.0 -F.cosine_similarity(x,y)), \n",
        "                                             margin = 1.0)\n",
        "optim = torch.optim.Adam(net.parameters(),lr=1e-4)\n",
        "for epoch in tqdm(range(300)):\n",
        "    all_error = 0\n",
        "    for ind,batch in enumerate(dl):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        anchor = batch[:,0,:].to(torch.float).reshape(1,batch.shape[0],batch.shape[2])\n",
        "        positive = batch[:,1,:].to(torch.float).reshape(1,batch.shape[0],batch.shape[2])\n",
        "        negative = batch[:,2,:].to(torch.float).reshape(1,batch.shape[0],batch.shape[2])\n",
        "        anchor = net(anchor)\n",
        "        positive = net(positive)\n",
        "        negative = net(negative)\n",
        "\n",
        "        loss = criterion(anchor, positive, negative)\n",
        "        all_error += loss\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    \n",
        "\n",
        "    if epoch%10 == 0:\n",
        "      print(all_error/length_sample)\n",
        "    errors.append(all_error/length_sample)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}